{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yejijang-analyst/ESAA/blob/main/exercise/24-1/week2_Traininglinearmodels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 모델 훈련 연습 문제**\n",
        "___\n",
        "- 출처 : 핸즈온 머신러닝 Ch04 연습문제 1, 5, 9, 10\n",
        "- 개념 문제의 경우 텍스트 셀을 추가하여 정답을 적어주세요."
      ],
      "metadata": {
        "id": "zCu72vDHGMHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?**\n",
        "___\n"
      ],
      "metadata": {
        "id": "j3g-_Dq9GiuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "미니배치 , SGD, 배치 GD 를 활용해볼 수 있다."
      ],
      "metadata": {
        "id": "MDpD--P3fHVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?**\n",
        "___"
      ],
      "metadata": {
        "id": "-pDjW5XcHPOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr 이 너무 크게되어있을 확률이 높으니 낮게 조절해보는 것이 필요할듯. 훈련에러도 올라가는지 확인하여 맞으면 lr 조정."
      ],
      "metadata": {
        "id": "m281gwhtfH8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 릿지 회귀를 사용했을 때 훈련 오차가 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $\\alpha$를 증가시켜야 할까요 아니면 줄여야 할까요?**\n",
        "___"
      ],
      "metadata": {
        "id": "nM7JbsLoy7b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 오차와 검증 오차가 거의 비슷하고 매우 높다면 모델이 훈련 세트에 과소적합되었을 가능성이 높다. 즉, 높은 편향을 가진 모델이다. 따라서 규제 하이퍼파라미터 α를 감소시켜야한다. α를 증가시킬수록 복잡도가 떨어짐. α를 낮춰야 분산이 증가하고 편향이 감소함.\n"
      ],
      "metadata": {
        "id": "V1xQWJFygsbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. 다음과 같이 사용해야 하는 이유는?**\n",
        "___\n",
        "- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀\n",
        "- 릿지 회귀 대신 라쏘 회귀\n",
        "- 라쏘 회귀 대신 엘라스틱넷"
      ],
      "metadata": {
        "id": "C8tARu-ZzOGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "평범한 선형 회귀 (아무런 규제가 없음) 대신 릿지 회귀: 규제가 있는 모델이 규제가 없는 모델에 비해 성능이 좋다. 그래서 평범한 회귀보다는 릿지 회귀가 선호된다.\n",
        "\n",
        "릿지 회귀 대신 라쏘 회귀: 라쏘 회귀는 l1 패널티를 사용하여 가중치를 완전히 0으로 만드는 경향이 있다. 가장 중요한 가중치를 제외하고는 모두 0이 되는 희소한(Sparse)한 모델을 만든다. 그리고 \"자동으로 특성 선택\"의 효과를 가지므로 단지 몇 개의 특성만 실제 유용할 것이라고 의심될 때 사용하면 좋다. 확신이 없다면 릿지 회귀를 사용해야 합니다.\n",
        "\n",
        "라쏘회귀 대신 엘라스틱넷: 몇 개의 특성이 강하게 연관되어 있거나 / 특성 수가 훈련 샘플보다 많을 때 라쏘가 불규칙적으로 행동하여 엘라스틱넷이 더 선호된다. 하지만 추가적인 하이퍼파라미터가 생기게 된다. 불규칙한 행동이 없는 라쏘를 원하면 엘라스틱넷에 l1_ratio를 1에 가깝게 하면 된다. (ratio = 0(Ridge) ~ 1(Lasso))\n"
      ],
      "metadata": {
        "id": "WvyOU9a9g5Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **추가) 조기 종료를 사용한 배치 경사 하강법으로 iris 데이터를 활용해 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요)**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QIZpOEYJVIAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8pXDQ_fU8Nz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "list(iris.keys())"
      ],
      "metadata": {
        "id": "ISEtQGYSigqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = iris.data[:, (2,3)] # 꽃잎 길이, 꽃잎 너비\n",
        "y = iris[\"target\"]\n",
        ""
      ],
      "metadata": {
        "id": "y7oIBDx-kKSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "X_b = np.c_[np.ones(len(X)), X]"
      ],
      "metadata": {
        "id": "cdgenk8XkKYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_b, y, test_size = 0.2, random_state=42)"
      ],
      "metadata": {
        "id": "keeLTN-7kKeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(categories='auto')\n",
        "y_train_ohe = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
        "y_test_ohe = encoder.fit_transform(y_test.reshape(-1, 1)).toarray()\n",
        ""
      ],
      "metadata": {
        "id": "fHu5pKBBkRXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "  exps = np.exp(z)\n",
        "  exp_sum = np.sum(exps, axis=1, keepdims = True)\n",
        "  return exps/exp_sum"
      ],
      "metadata": {
        "id": "-oz5q_tgkRUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.1 # 학습률\n",
        "n_epochs = 1000\n",
        "m = len(X_train) # 샘플 개수\n",
        "history=[]\n",
        "epsilon = 1e-7\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  Y_proba = softmax(s_x)\n",
        "  loss = -np.mean(np.sum(y_train_ohe * np.log(Y_proba + epsilon), axis=1))\n",
        "  error = Y_proba - y_train_ohe # (#,3)\n",
        "  gradients = (1/m) * (X_train.T.dot(error))\n",
        "  theta = theta - eta * gradients\n",
        "  history.append(loss)"
      ],
      "metadata": {
        "id": "fRzQmWQxkRRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDLGOvo5kRPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wgQ-QesvkRMm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}